model:
  params:
    max_seq_len: 64
    self_condition: True
    unconditional_prob: 0.1
  diffusion:
    params:
      timesteps: 1000
      sampling_timesteps: 250
      loss_type: "l2"
      objective: "pred_noise"
      beta_schedule: "cosine"
      dim_head: 64
      p2_loss_weight_k: 1
      p2_loss_weight_gamma: 0.0
      ddim_sampling_eta: 1.0
      normalize_latent: True
      scale_factor: 0.121005
      scale_mean: 0.002565
      use_ema: True
      ema_decay: 0.9999
      self_condition: True
      enc_dec_model: "huggingface/bart-base"
      unconditional_prob: 0.1
      learning_rate: 1.0e-4
      scheduler: "cosine"
      lr_warmup_steps: 0
      num_train_steps: 60000
  transformer:
    params:
      depth: 6
      latent_dim: 768
      attention_head_dim: 64
      dropout: 0.1
      scale_shift: False
  sample:
    beam:
      max_length: 64
      min_length: 5
      do_sample: False
      num_beams: 4
      no_repeat_ngram_size: 3
      repetition_penalty: 1.2
    nucleus:
      max_length: 64
      min_length: 5
      do_sample: True
      top_p: 0.95
      num_beams: 1
      no_repeat_ngram_size: 3
      repetition_penalty: 1.2
data:
  name: "qqp"
  params:
    tokenizer: "huggingface/bart-base"
    max_token_len: 64
    batch_size: 256
    num_workers: 4
train:
  name: ""
  output_dir: "saved_models"
  params:
    precision: 32
    learning_rate: 1.0e-4
    lr_warmup_steps: 0
    num_train_steps: 60000
    accumulate_grad_batches: 1