model:
  params:
    max_seq_len: 96
    unconditional_prob: 0.1
  diffusion:
    params:
      timesteps: 1000
      sampling_timesteps: 250
      loss_type: "l1"
      parameterization: "x0"
      beta_schedule: "cosine"
      condition_key: 'crossattn'
      ddim_sampling_eta: 1.0
      normalize: True
      scale_factor: 0.123292
      scale_mean: 0.002169
      use_ema: True
      learn_logvar: False
      ema_decay: 0.9999
      self_condition: True
      enc_dec_model: "huggingface/finetune/twitter/bart-base"
      unconditional_prob: 0.1
      learning_rate: 1.0e-4
  transformer:
    params:
      tx_depth: 12
      tx_dim: 768
      latent_dim: 768
      attention_head_dim: 64
      max_seq_len: 128
      dropout: 0.1
      scale_shift: True
  sample:
    beam:
      max_length: 96
      min_length: 5
      do_sample: False
      num_beams: 4
      no_repeat_ngram_size: 3
      repetition_penalty: 1.2
    nucleus:
      max_length: 96
      min_length: 5
      do_sample: True
      top_p: 0.95
      num_beams: 1
      no_repeat_ngram_size: 3
      repetition_penalty: 1.2
data:
  name: "twitter"
  params:
    tokenizer: "huggingface/bart-base"
    max_token_len: 96
    batch_size: 64
    num_workers: 4
train:
  name: ""
  output_dir: "saved_models"
  params:
    precision: 16
    learning_rate: 1.0e-4
    lr_warmup_steps: 0
    num_train_steps: 400000
    accumulate_grad_batches: 1